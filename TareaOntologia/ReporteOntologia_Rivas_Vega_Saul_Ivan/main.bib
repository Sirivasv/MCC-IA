@article{purwins_deep_2019,
	title = {Deep {Learning} for {Audio} {Signal} {Processing}},
	volume = {13},
	url = {https://arxiv.org/pdf/1905.00078.pdf},
	issn = {1932-4553},
	doi = {10.1109/JSTSP.2019.2908700},
	abstract = {Given the recent surge in developments of deep learning, this paper provides a review of the state-of-the-art deep learning techniques for audio signal processing. Speech, music, and environmental sound processing are considered side-by-side, in order to point out similarities and differences between the domains, highlighting general methods, problems, key references, and potential for cross fertilization between areas. The dominant feature representations (in particular, log-mel spectra and raw waveform) and deep learning models are reviewed, including convolutional neural networks, variants of the long short-term memory architecture, as well as more audio-specific neural network models. Subsequently, prominent deep learning application areas are covered, i.e., audio recognition (automatic speech recognition, music information retrieval, environmental sound detection, localization and tracking) and synthesis and transformation (source separation, audio enhancement, generative models for speech, sound, and music synthesis). Finally, key issues and future questions regarding deep learning applied to audio signal processing are identified.},
	number = {2},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Purwins, H. and Li, B. and Virtanen, T. and Schlüter, J. and Chang, S. and Sainath, T.},
	month = may,
	year = {2019},
	keywords = {audio enhancement, audio recognition, audio signal processing, audio-specific neural network models, automatic speech recognition, Computational modeling, connectionist temporal memory, Convolution, convolutional neural nets, convolutional neural networks, Deep learning, environmental sound detection, environmental sound processing, environmental sounds, feature extraction, Hidden Markov models, information retrieval, learning (artificial intelligence), music, Music, music information retrieval, prominent deep learning application areas, source separation, speech recognition, state-of-the-art deep learning techniques, Task analysis},
	pages = {206--219},
	file = {08678825.pdf:/home/sirivasv/Documents/Research Library/08678825.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sirivasv/Zotero/storage/SPT8K3FA/8678825.html:text/html;IEEE Xplore Full Text PDF:/home/sirivasv/Zotero/storage/IB8QK5G5/Purwins et al. - 2019 - Deep Learning for Audio Signal Processing.pdf:application/pdf}
}
@article{ontology_101,
	title = {Ontology {Development} 101: {A} {Guide} to {Creating} {Your} {First} {Ontology}},
	language = {en},
	author = {Noy, Natalya F and McGuinness, Deborah L},
	pages = {25},
	file = {Noy and McGuinness - Ontology Development 101 A Guide to Creating Your.pdf:/home/sirivasv/Zotero/storage/BV9A9AWG/Noy and McGuinness - Ontology Development 101 A Guide to Creating Your.pdf:application/pdf}
}

@book{noauthor_computational_2017,
	address = {New York, NY},
	author = {Virtanen, Tuomas and Plumbley, Mark and Dan Ellis},
	edition = {1st edition},
	title = {Computational analysis of sound scenes and events},
	isbn = {978-3-319-63449-4},
	language = {en},
	publisher = {Springer Science+Business Media},
	year = {2017},
	file = {2017 - Computational analysis of sound scenes and events.pdf:/home/sirivasv/Zotero/storage/UIEKDGAH/2017 - Computational analysis of sound scenes and events.pdf:application/pdf;2018_Book_ComputationalAnalysisOfSoundSc.pdf:/home/sirivasv/Documents/Research Library/2018_Book_ComputationalAnalysisOfSoundSc.pdf:application/pdf}
}

@inproceedings{eck_finding_2002,
	address = {Martigny, Switzerland},
	title = {Finding temporal structure in music: blues improvisation with {LSTM} recurrent networks},
	isbn = {978-0-7803-7616-8},
	shorttitle = {Finding temporal structure in music},
	url = {http://ieeexplore.ieee.org/document/1030094/},
	doi = {10.1109/NNSP.2002.1030094},
	abstract = {Few types of signal streams are as ubiquitous as music. Here we consider the problem of extracting essential ingredients of music signals, such as well-deﬁned global temporal structure in the form of nested periodicities (or meter). Can we construct an adaptive signal processing device that learns by example how to generate new instances of a given musical style? Because recurrent neural networks can in principle learn the temporal structure of a signal, they are good candidates for such a task. Unfortunately, music composed by standard recurrent neural networks (RNNs) often lacks global coherence. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long Short-Term Memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing \& counting and learning of context sensitive languages. In the current study we show that LSTM is also a good mechanism for learning to compose music. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.},
	language = {en},
	urldate = {2019-08-26},
	booktitle = {Proceedings of the 12th {IEEE} {Workshop} on {Neural} {Networks} for {Signal} {Processing}},
	publisher = {IEEE},
	author = {Eck, D. and Schmidhuber, J.},
	year = {2002},
	pages = {747--756},
	file = {2002_ieee.pdf:/home/sirivasv/Documents/Research Library/2002_ieee.pdf:application/pdf}
}

@inproceedings{turpault_semi-supervised_2019,
	address = {Brighton, United Kingdom},
	title = {Semi-supervised {Triplet} {Loss} {Based} {Learning} of {Ambient} {Audio} {Embeddings}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683774/},
	doi = {10.1109/ICASSP.2019.8683774},
	abstract = {Deep neural networks are particularly useful to learn relevant representations from data. Recent studies have demonstrated the potential of unsupervised representation learning for ambient sound analysis using various ﬂavors of the triplet loss. They have compared this approach to supervised learning. However, in real situations, it is common to have a small labeled dataset and a large unlabeled one. In this paper, we combine unsupervised and supervised triplet loss based learning into a semi-supervised representation learning approach. We propose two ﬂavors of this approach, whereby the positive samples for those triplets whose anchors are unlabeled are obtained either by applying a transformation to the anchor, or by selecting the nearest sample in the training set. We compare our approach to supervised and unsupervised representation learning as well as the ratio between the amount of labeled and unlabeled data. We evaluate all the above approaches on an audio tagging task using the DCASE 2018 Task 4 dataset, and we show the impact of this ratio on the tagging performance.},
	language = {en},
	urldate = {2019-08-08},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Turpault, Nicolas and Serizel, Romain and Vincent, Emmanuel},
	month = may,
	year = {2019},
	pages = {760--764},
	file = {ssl_triplet.pdf:/home/sirivasv/Documents/Research Library/ssl_triplet.pdf:application/pdf}
}

@inproceedings{benetos_detection_2016,
	address = {Shanghai},
	title = {Detection of overlapping acoustic events using a temporally-constrained probabilistic model},
	isbn = {978-1-4799-9988-0},
	url = {http://ieeexplore.ieee.org/document/7472919/},
	doi = {10.1109/ICASSP.2016.7472919},
	abstract = {In this paper, a system for overlapping acoustic event detection is proposed, which models the temporal evolution of sound events. The system is based on probabilistic latent component analysis, supporting the use of a sound event dictionary where each exemplar consists of a succession of spectral templates. The temporal succession of the templates is controlled through event class-wise Hidden Markov Models (HMMs). As input time/frequency representation, the Equivalent Rectangular Bandwidth (ERB) spectrogram is used. Experiments are carried out on polyphonic datasets of ofﬁce sounds generated using an acoustic scene simulator, as well as real and synthesized monophonic datasets for comparative purposes. Results show that the proposed system outperforms several state-of-the-art methods for overlapping acoustic event detection on the same task, using both frame-based and event-based metrics, and is robust to varying event density and noise levels.},
	language = {en},
	urldate = {2019-08-08},
	booktitle = {2016 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Benetos, Emmanouil and Lafay, Gregoire and Lagrange, Mathieu and Plumbley, Mark D.},
	month = mar,
	year = {2016},
	pages = {6450--6454},
	file = {event_detection_icassp16-8.pdf:/home/sirivasv/Documents/Research Library/event_detection_icassp16-8.pdf:application/pdf}
}

@inproceedings{gemmeke_audio_2017,
	address = {New Orleans, LA},
	title = {Audio {Set}: {An} ontology and human-labeled dataset for audio events},
	isbn = {978-1-5090-4117-6},
	shorttitle = {Audio {Set}},
	url = {https://ai.google/research/pubs/pub45857.pdf},
	doi = {10.1109/ICASSP.2017.7952261},
	abstract = {Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous beneﬁts from comprehensive datasets – principally ImageNet. This paper describes the creation of Audio Set, a largescale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of speciﬁc audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.},
	language = {en},
	urldate = {2019-09-20},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
	month = mar,
	year = {2017},
	pages = {776--780},
	file = {45857.pdf:/home/sirivasv/Documents/Research Library/45857.pdf:application/pdf;Gemmeke et al. - 2017 - Audio Set An ontology and human-labeled dataset f.pdf:/home/sirivasv/Zotero/storage/H3K3LP65/Gemmeke et al. - 2017 - Audio Set An ontology and human-labeled dataset f.pdf:application/pdf}
}
@book{rae_2014, place={[Madrid]}, edition={Vigesimotercera edición, Edición del Tricentenario}, title={Diccionario de la lengua española}, url={http://dle.rae.es/?id=DgIqVCc}, publisher={Real Academia Española}, author={Real Academia Española and Asociación de Academias de la Lengua Española}, year={2014} }